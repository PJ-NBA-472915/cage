{
  "id": "2025-10-17-add-local-ollama-embeddings",
  "title": "Add local Ollama embedding provider with bge-code-v1",
  "description": "Add runtime-configurable embedding provider to switch between OpenAI and local Ollama-hosted BAAI/bge-code-v1 model for RAG indexing",
  "status": "pending",
  "priority": "high",
  "tags": ["rag", "embeddings", "ollama", "infrastructure", "cost-optimization"],
  "created_at": "2025-10-17T10:20:00Z",
  "updated_at": "2025-10-17T10:20:00Z",
  "acceptance_criteria": [
    "AC-1: Setting EMBEDDING_PROVIDER=openai|local cleanly switches embedding source without code changes elsewhere",
    "AC-2: With local, the indexer successfully retrieves vectors from Ollama's /api/embeddings using bge-code-v1 and stores them in pgvector with correct dimension",
    "AC-3: Failure to reach Ollama yields clear, actionable error and process exits non-zero",
    "AC-4: Metrics show per-provider request counts and average latency",
    "AC-5: Documentation explains model provisioning and environment configuration for DEV/CI/PROD"
  ],
  "tasks": [
    {
      "id": "1",
      "description": "Add Ollama service to docker-compose.yml",
      "status": "pending",
      "details": [
        "Add ollama service with ollama/ollama:latest image",
        "Expose port 11434",
        "Add volume ollama_models:/root/.ollama for model persistence",
        "Configure healthcheck using /api/tags endpoint",
        "Set OLLAMA_KEEP_ALIVE=24h environment variable"
      ]
    },
    {
      "id": "2",
      "description": "Add new environment variables for embedding provider configuration",
      "status": "pending",
      "details": [
        "EMBEDDING_PROVIDER=openai|local (default: openai)",
        "EMBEDDING_MODEL_OPENAI=text-embedding-3-small",
        "EMBEDDING_MODEL_LOCAL=bge-code-v1",
        "OLLAMA_BASE_URL=http://ollama:11434",
        "EMBED_DIM_OVERRIDE (optional for validation)"
      ]
    },
    {
      "id": "3",
      "description": "Create EmbeddingAdapter interface and implementations",
      "status": "pending",
      "details": [
        "Define abstract EmbeddingAdapter base class with embed() and name() methods",
        "Implement OpenAIEmbeddingAdapter (refactor existing code)",
        "Implement OllamaEmbeddingAdapter with /api/embeddings integration",
        "Add optional L2 normalization for Ollama embeddings",
        "Create factory function makeEmbeddingAdapter() based on EMBEDDING_PROVIDER"
      ]
    },
    {
      "id": "4",
      "description": "Update RAG service to use embedding adapter",
      "status": "pending",
      "details": [
        "Replace direct OpenAI calls with adapter.embed()",
        "Validate embedding dimension on startup",
        "Store dimension metadata per provider/model",
        "Update pgvector schema if dimension differs (bge-code-v1 is 768-dim)",
        "Add migration path for existing embeddings table"
      ]
    },
    {
      "id": "5",
      "description": "Add model provisioning script/documentation",
      "status": "pending",
      "details": [
        "Create script to pull mahonzhan/bge-code-v1 from Ollama",
        "Document alternative: custom GGUF with Modelfile",
        "Document alternative: using bge-m3 from official Ollama library",
        "Add Makefile target: make ollama-pull-model",
        "Add startup check in rag-api to verify model availability"
      ]
    },
    {
      "id": "6",
      "description": "Add health checks and observability",
      "status": "pending",
      "details": [
        "Startup check: verify Ollama connectivity and model availability",
        "Add metrics: embedding_requests_total{provider}",
        "Add metrics: embedding_failures_total{provider}",
        "Add metrics: avg_latency_ms{provider}",
        "Add gauge: embedding_dim{provider,model}",
        "Log embedding provider and model on service startup"
      ]
    },
    {
      "id": "7",
      "description": "Update pgvector schema for variable dimensions",
      "status": "pending",
      "details": [
        "bge-code-v1 produces 768-dimensional vectors (vs OpenAI's 1536)",
        "Add provider column to embeddings table",
        "Consider: separate tables per provider OR polymorphic column",
        "Update indices for new dimension",
        "Add migration to handle existing OpenAI embeddings"
      ]
    },
    {
      "id": "8",
      "description": "Add integration tests for Ollama adapter",
      "status": "pending",
      "details": [
        "Test basic embedding generation via Ollama",
        "Test batch embedding (multiple chunks)",
        "Test error handling when Ollama unreachable",
        "Test dimension validation",
        "Compare search quality: local vs OpenAI on sample queries"
      ]
    },
    {
      "id": "9",
      "description": "Add documentation and configuration guide",
      "status": "pending",
      "details": [
        "Update CLAUDE.md with Ollama configuration",
        "Document model provisioning steps",
        "Document environment variable configuration",
        "Add troubleshooting guide for common issues",
        "Document rollout/rollback procedure",
        "Add cost comparison: OpenAI vs local",
        "Document performance/quality tradeoffs"
      ]
    },
    {
      "id": "10",
      "description": "Implement rollout plan with canary testing",
      "status": "pending",
      "details": [
        "Phase 1: Test on small subset (1 repo)",
        "Measure recall@k on 15-20 real queries vs OpenAI baseline",
        "Optional: dual-write both embeddings for comparison",
        "Phase 2: Promote to default if quality >= baseline",
        "Document rollback procedure (env flip to openai)"
      ]
    }
  ],
  "technical_notes": {
    "embedding_dimensions": {
      "openai_text-embedding-3-small": 1536,
      "bge-code-v1": 768
    },
    "ollama_api": {
      "embeddings_endpoint": "/api/embeddings",
      "tags_endpoint": "/api/tags",
      "health_check": "GET /api/tags"
    },
    "model_options": [
      "mahonzhan/bge-code-v1 (community pull)",
      "Custom GGUF with Modelfile (Q8_0/Q4_K_M quantization)",
      "bge-m3 from official Ollama library (alternative)"
    ],
    "risks": [
      "Dimension drift between providers requires schema management",
      "Model availability - need locked tags or shipped GGUF",
      "CPU throughput may be slow - consider GPU runtime or replicas",
      "First-run model pull adds startup time"
    ]
  },
  "dependencies": [],
  "related_files": [
    "src/cage/rag_service.py",
    "src/apps/rag_api/main.py",
    "docker-compose.yml",
    "scripts/init-db.sql",
    "CLAUDE.md"
  ],
  "estimated_effort": "2-3 days",
  "assignee": null
}
